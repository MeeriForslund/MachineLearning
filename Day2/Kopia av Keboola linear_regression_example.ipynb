{"cells":[{"cell_type":"markdown","metadata":{"id":"ajHlHFqEecO7"},"source":["# Get This Notebook\n","\n","Clik on `file` (upper left corner) and choose from the dropdown the option, which you prefer:\n","- `Save a copy in Drive` to add the notebook to your Google Drive (where you can access it via Google Colab)\n","- `Download .ipynb` to download the Notebook as a Jupyter Notebook, which you can run from your local machine"]},{"cell_type":"markdown","metadata":{"id":"V0cl9EKl3f-3"},"source":["# Before we start: Setup\n","\n","Before we start coding out the solution, make sure you have all the prerequisite Python packages. Run the following code to install the versions of the packages you will be needing for this Notebook."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":238},"executionInfo":{"elapsed":5603,"status":"ok","timestamp":1597072277397,"user":{"displayName":"Teo Radetic","photoUrl":"","userId":"17402841699569995409"},"user_tz":-120},"id":"d3cAbyOQ0y4i","outputId":"f71a38e2-50f4-49bd-8e47-b051ea98e3b6"},"outputs":[],"source":["# Install necessary pip packages in the current Jupyter kernel\n","import sys\n","!{sys.executable} -m pip install pandas==1.0.5 matplotlib==3.2.2 seaborn==0.10.1 numpy==1.18.5 scikit-learn==0.22.2"]},{"cell_type":"markdown","metadata":{"id":"Dp25UC3Z3MK4"},"source":["# Problem Description: Predict price of real estate\n","\n","You are given a dataset with with historical information about real estate valuation. Your job is to build a linear regression model, which can predict the housing valuation based on the estate's attributes.\n","\n","The dataset can be [found here](https://archive.ics.uci.edu/ml/datasets/Real+estate+valuation+data+set).\n","</br>\n","</br>\n","\n","### Attribute Information\n","\n","The inputs are as follows</br>\n","X1=the transaction date (for example, 2013.250=2013 March, 2013.500=2013 June, etc.)</br>\n","X2=the house age (unit: year)</br>\n","X3=the distance to the nearest MRT station (unit: meter)</br>\n","X4=the number of convenience stores in the living circle on foot (integer)</br>\n","X5=the geographic coordinate, latitude. (unit: degree)</br>\n","X6=the geographic coordinate, longitude. (unit: degree)</br>\n","</br>\n","The output is as follow:</br>\n","Y= house price of unit area (10000 New Taiwan Dollar/Ping, where Ping is a local unit, 1 Ping = 3.3 meter squared)"]},{"cell_type":"markdown","metadata":{"id":"Xi3Aa6Ebgz4n"},"source":["# STEP 1: Load & inspect data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":221},"executionInfo":{"elapsed":2016,"status":"ok","timestamp":1597149130200,"user":{"displayName":"Teo Radetic","photoUrl":"","userId":"17402841699569995409"},"user_tz":-120},"id":"m59Mfufv1duA","outputId":"66217466-e411-4af4-8dc6-f2d37ef02e6b"},"outputs":[],"source":["import pandas as pd\n","\n","dataset_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00477/Real%20estate%20valuation%20data%20set.xlsx'\n","df = pd.read_excel(dataset_url)\n","\n","# check the first 5 rows to get a feeling of the data\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"BNwPfqAL5lvB"},"source":["# STEP 2: Exploratory Data Analysis (EDA) & Data Cleaning\n","Inspect and visualize the data to notice trends.\n","\n","Clean the data according to [best practices](https://www.keboola.com/blog/the-ultimate-guide-to-data-cleaning)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":331},"executionInfo":{"elapsed":615,"status":"ok","timestamp":1597149328913,"user":{"displayName":"Teo Radetic","photoUrl":"","userId":"17402841699569995409"},"user_tz":-120},"id":"L9HVvNzTg9y7","outputId":"ea1c04c9-8b72-4306-eb5a-129a3af4d747"},"outputs":[],"source":["# get descriptive statistics for each column/feature\n","df.describe(include='all')"]},{"cell_type":"markdown","metadata":{"id":"qBADiDED5SMO"},"source":["The information above is useful to understand the top down statistics of data (e.g. mean, median, standard deviation), but this works even better, if we visualize the data.\n","\n","Before we visualize it, though, let us clean it a bit.\n","\n","First, we remove features, which will not be used in the analysis."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"elapsed":681,"status":"ok","timestamp":1597149524362,"user":{"displayName":"Teo Radetic","photoUrl":"","userId":"17402841699569995409"},"user_tz":-120},"id":"wH-yk1hyh2e3","outputId":"b7a59143-bae5-4808-eb21-36c535aecea3"},"outputs":[],"source":["# remove columns which are unnecessary for linear regression\n","list_of_columns_to_drop = ['No',\n","                           'X1 transaction date',\n","                           'X5 latitude',\n","                           'X6 longitude']\n","\n","df.drop(list_of_columns_to_drop, axis=1, inplace=True)\n","\n","# check if columns dropped properly\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"HN1IjmQbh8-0"},"source":["Next, check if there are any **missing values** within the dataframe."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":102},"executionInfo":{"elapsed":610,"status":"ok","timestamp":1597149641488,"user":{"displayName":"Teo Radetic","photoUrl":"","userId":"17402841699569995409"},"user_tz":-120},"id":"UfxGA19liBN4","outputId":"10174789-d141-46b1-f8eb-c118dc19231e"},"outputs":[],"source":["# count the number of rows with missing values per column\n","df.isna().sum()"]},{"cell_type":"markdown","metadata":{"id":"e9ktugBtiZP1"},"source":["All good! No information is missing, so we do not have to remove anything at this moment.\n","\n","For the next step, we check data distribution, and whether there are any **outliers**. We do it by visualizing the data with box plots."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":594},"executionInfo":{"elapsed":983,"status":"ok","timestamp":1597149764191,"user":{"displayName":"Teo Radetic","photoUrl":"","userId":"17402841699569995409"},"user_tz":-120},"id":"wOQ_zNPF1eIk","outputId":"1deacd5a-8eae-4241-a2df-9200f1a11ddb"},"outputs":[],"source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","fig, axs = plt.subplots(ncols=2, nrows=2, figsize=(20, 10))\n","index = 0\n","axs = axs.flatten()\n","for k,v in df.items():\n","    sns.boxplot(y=k, data=df, ax=axs[index])\n","    index += 1\n","plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)"]},{"cell_type":"markdown","metadata":{"id":"mCRa3lIJi507"},"source":["From the Box plot we can see that both the house age and number of convenience stores look as if they are not having issues with outliers.\n","\n","Distance from the nearest MRT station and price of unit area (Y) seem to be both containing a couple of outliers.\n","\n","For a housing prediction it is expected that some houses will sell above the market price, so at this point we will not remove the outliers in the dependent variable (Y).\n","\n","But before we continue, let us check the **distribution** of each feature. As you can remember, linear regression assumes a **normal distribution** for each feature."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":594},"executionInfo":{"elapsed":2133,"status":"ok","timestamp":1597149935372,"user":{"displayName":"Teo Radetic","photoUrl":"","userId":"17402841699569995409"},"user_tz":-120},"id":"qdKYib-14ma3","outputId":"e711fabb-3376-417b-c5df-13dbd69a5905"},"outputs":[],"source":["fig, axs = plt.subplots(ncols=2, nrows=2, figsize=(20, 10))\n","index = 0\n","axs = axs.flatten()\n","for k,v in df.items():\n","    sns.distplot(v, ax=axs[index])\n","    index += 1\n","plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)"]},{"cell_type":"markdown","metadata":{"id":"iidMWM99jywg"},"source":["It seems data distribution is slightly problematic. X3 suffers from skewness and a long-tail. X2 seems to be bimodal instead of normal-shaped. The target variable Y, on the other hand, seems to be nicely normally shaped.\n","\n","For the time being we will just note these potential issues, and continue, with the note-to-ourselves to revisit these distributions in case we need to improve the linear regression model."]},{"cell_type":"markdown","metadata":{"id":"08qt-PVz5o0a"},"source":["## STEP 3: Train and evaluate linear regression model\n","\n","Start by splitting the data into a training set (for model fitting) and a testing set (for model evaluation)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U0_DdFalkx6V"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","# separate the features (X) from the target (Y)\n","Y = df['Y house price of unit area']\n","X = df[['X2 house age',\n","        'X3 distance to the nearest MRT station',\n","        'X4 number of convenience stores']]\n","\n","# divide the data into a training set (for learning) and test set (for validation)\n","x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"]},{"cell_type":"markdown","metadata":{"id":"QBjjYKEXnuNX"},"source":["Train the linear regression model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hXuGQVZ1lNk5"},"outputs":[],"source":["from sklearn.linear_model import LinearRegression\n","\n","reg_model = LinearRegression()\n","trained_model = reg_model.fit(x_train, y_train)"]},{"cell_type":"markdown","metadata":{"id":"RXJwTlHQoFyw"},"source":["Use the model to predict new data and evaluate its performance."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wx7zzkEDn5qj"},"outputs":[],"source":["# make predictions\n","y_predicted = trained_model.predict(x_test)\n","\n","# rename y_test into y_true for clarity\n","y_true = y_test"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":85},"executionInfo":{"elapsed":1705,"status":"ok","timestamp":1597151583798,"user":{"displayName":"Teo Radetic","photoUrl":"","userId":"17402841699569995409"},"user_tz":-120},"id":"UkU4Kei_oUdC","outputId":"ec3281c3-8a55-43b3-e9f9-71d7d93dceb7"},"outputs":[],"source":["from sklearn.metrics import mean_squared_error\n","\n","# get evaluation metrics\n","mse = round(mean_squared_error(y_true, y_predicted), 2)\n","rmse = round(mean_squared_error(y_true, y_predicted, squared=False), 2)\n","r2 = round(trained_model.score(x_test, y_test), 2)\n","\n","# evaluate performance\n","print(\"Model performance:\\nMSE: {}\\nRMSE: {}\\nR2 score: {}\".format(mse, rmse, r2))"]},{"cell_type":"markdown","metadata":{"id":"26cCjGdqp0Af"},"source":["**Interpretation**: </br>\n","The model captures some variance in the target variable (R2 score 65%), but could be massively improved.\n","\n","# Your turn\n","Improve the model, to perform better than the baseline above.\n","\n","A couple of things you can explore:\n","1. Does the model performance improve once we remove outliers?\n","2. How does feature normalization affect model performance?\n","3. Add other features (e.g. use longitude and latitude to determine in which area the house is located, bin distance to nearest MRT station to check if there is a signal in the further place, ...). How do new features affect performance?"]}],"metadata":{"colab":{"provenance":[{"file_id":"1UDnGtKMCIwDtbjQt6cCdT_DFmRkNiJhG","timestamp":1724358331192},{"file_id":"1pkarXTsZfAK2vS4d1_GZ8y2VduehyXlN","timestamp":1597222248759}]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}
